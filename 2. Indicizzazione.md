La prima attività necessaria per il funzionamento di un IRS è la generazione di un archivio di documenti. Si tratta di una fase che avviene tipicamente offline e attraverso i seguenti passaggi:
1. localizzazione, l'inserimento manuale, semi-automatico o automatico dei documenti alla collezione;
2. decodifica del formato, la trasformazione del documento in stringhe;
3. indicizzazione, la creazione di una rappresentazione utilizzabile in maniera più efficiente del contenuto di un documento;
4. generazione struttura dati, la memorizzazione dei della struttura dai prodotta durante la fase precedente e dei documenti.

L'indicizzazione di un documento testuale è il processo che ne esamina il contenuto informativo, producendo un'opportuna lista di termini indice, molto spesso pesati. I termini indice sono definiti come gli elementi base della rappresentazione formale di un documento o di una query, dal punto di vista pratico possono essere: parole o radici di parole o intere frasi estratte da documento oppure parole estratte da un vocabolario controllato. La lista dei termini indice prodotta dalla fase di indicizzazione rappresenta una versione più compatta del documento di partenza. Il processo di indicizzazione si compone di 5 fasi, anche se non è necessariamente detto che tutti i sistemi di IR vadano effettivamente ad implementarle:
1. analisi lessicale e selezione delle parole;
2. rimozione delle stopwords;
3. riduzione delle parole alle rispettivi radici (stemming);
4. pesature degli elementi indice;
5. compressione.

L'analisi lessicale e la selezione delle parole rappresentano un processo di trasformazione di un flusso di caratteri del testo originario del documento, visto a livello macchina come un flusso di caratteri, in un flusso di tokens, ossia sequenze di caratteri (spesso parole) con un significato specifico attraverso l'identificazione di caratteri ritenuti di separazione. (quello che viene fatto dai tokenizer in Lucene)

Il processo di rimozione delle stopwords elimina le parole molto frequenti in tutti i documenti, e quindi di scarso contenuto informativo, consentendo una riduzione del 30-50% del numero di tokens. Questo processo può essere svolto sia andando a ricercare i termini funzionali (articoli, preposizioni, ...) per una data lingua, noti a priori, utilizzando un dizionario, oppure a partire dall'analisi statistica della frequenza dei termini nella collezione.

Lo stemming è un processo che riduce tutti le parole con la stessa radice (in inglese, stem) ad un unico termine. L'assunzione alla base di questo processo è che le parole con la stessa radice possano avere la stessa origine etimologica, e quindi un contenuto informativo molto simile, cosa vera in molte lingue.

Una volta terminato il processo di indicizzazione di una collezione di documenti, il risultato è rappresentato da una matrice sparsa, in cui i pesi associati ad un termine indice possono essere valori binari, reali o interi positivi.

Le caratteristiche che si ricercano nei termini indice prodotti per un documento sono esaustività, in quanto devono esprimere quanto più possibile del contenuto informativo del documento, e specificità, in quanto devono consentire il più possibile di distinguere il documento dagli altri.

I termini la cui frequenza è alta in tutti i documenti prendono il nome di termini funzionali. I termini che identificano il contenuto del documento e hanno una frequenza variabile da un documento all'altro vengono detti indicatori del contenuto di un documento.

Un tesauro è una sorta di "mappa" per un linguaggio o per una sotto-parte di un linguaggio, nel caso di tesauri tematici. (esempio, lessico specifico di una particolare disciplina).

Si possono distinguere 3 diverse tipologie di tesauro: gerarchici, clustered e associativi.

I tesauri gerarchici rappresentano le relazioni tra i termini da loro contenuti, con particolare attenzione a quelle di gerarchia. I tesauri di questo tipo vengono utilizzati in fase di espansione delle query e degli indici. Le grandi difficoltà legate a questa tipologia di tesauri sono rappresentati dalle fasi di generazione, in quanto questa necessita di esperti del settore e può essere svolta esclusivamente in maniera manuale, e di manutenzione, la quale si rivela necessaria al fine di mantenere il tesauro in linea con la continua evoluzione del linguaggio ed è anch'essa caratterizzata dagli stessi problemi della fase di generazione.

Un tesauro clustered rappresenta un grafo contenente gruppi di parole in cui due gruppi sono connessi se tra di essi è definita una relazione semantica. Un esempio di tesauro di questo tipo, per la lingua inglese, è Wordnet. A differenza della tipologia precedente questi tesauri possono essere generati in maniera automatica.

L'attività di clustering consiste in un'attività di raggruppamento dei documenti appartenenti alla collezione in classi all'interno delle quali si trovano documenti simili tra di loro. Si parla di clustering globale in caso di raggruppamenti basati sulla co-occorrenza degli indice nell'intera collezione, di clustering locale se effettuato sulla base del contesto della query.

Infine, i tesauri associativi, o pseudo-tesauri, rappresentano dei sottogruppi di termini utilizzando relazioni gerarchiche di similarità e associando peso e verso ad ognuna di queste. Uno dei vantaggi di questa tipologia di tesauri è rappresentato dal fatto che la loro costruzione può essere automatizzata sfruttando: una matrice di similarità e una funzione soglia, entrambi definite a partire dalla co-occorrenza e co-assenza dei termini all'interno dei documenti appartenenti alla collezione.

Essendo che non tutte le parole di un documento lo descrivono con la stessa precisione, è possibile associare i termini indice ai documenti di una collezione con un dato peso, il quale verrà utilizzato per tenere conto della significatività del termine nel documento. La più semplice funzione di pesatura che si possa definire associa un peso uguale a 1 nel caso in cui il termine sia presente all'interno del documento, 0 in caso contrario. Si tratta come detto di una funzione molto semplice, che non tiene conto però, ad esempio, della frequenza del termine all'interno del documento.

Per ogni termine associato ad un documento è possibile definire due funzioni: frequenza e rank. La frequenza indica la frequenza con cui il termine compare all'interno del documento, il rank indica la posizione del termine all'interno della lista dei termini, presenti nel documento, ordinata per frequenza. Il valore del prodotto tra rank e frequenza è costante per tutti i termini di una stessa collezione.

La curva di Zipf rappresenta una funzione che descrive il potere discriminante dei termini all'interno di una collezione andando a presentare il rapporto tra frequenza e rank dei termini. Si osserva che questa curva presenta due forti pendenze in corrispondenza dei termini più frequenti (upper cut-off) e dei termini meno frequenti (lower cut-off). La capacità dei termini di discriminare il contenuto dei documenti è massima nella posizione intermedia tra i due livelli di cut-off. (Analisi di Luhn)

Molti criteri di indicizzazione sono basati sull'analisi di Luhn, in particolare:
- pesatura dei termini indici, associa un peso minore alle parole più frequenti;
- stop lists, elimina dagli indici i termini più frequenti (upper cut-off);
- parole significative, elimina dagli indici i termini sia i termini più frequenti che quelli meno frequenti (upper cut-off + lower cut-off).

La significatività $w$ di un documento è una funzione composta da due fattori: $w_{td} = w_{td} * discr_t$, dove $f_{td}$ rappresenta la frequenza del termine $t$ nel documento $d, ed è in relazione alla esaustività (fattore di recall), mentre $discr_t$, è in relazione alla specificità (fattore di precision).

L'Inverse Document Frequency, o IDF, di un termine $t$ è definita come $discr_t = idf_t = log \frac{N}{df_t}$, dove $d_f$ è il numero di documenti in cui il termine $t_j$ appare e $N£ è il numero di documenti della collezione.

A partire da queste misure, il peso $w_{ij}$ del termine $t_i$ in un documento $d_j$ è definito come $w_{ij} = tf_{ij} \times log \frac{N}{df_t}$. Il peso viene associato ad ogni termine in base al documento in analisi e viene calcolato solamente dopo aver eliminato i termini funzionali. Essendo che la frequenza assoluta $tf_{ij}$ di un termine cresce all'aumentare della lunghezza del documento $d_j$, il peso viene normalizzato utilizzando la seguente formula $w_{ij} = \frac{tf_{ij}}{max\ tf_{j}} \times log \frac{N}{df_i}$, dove $max\ tf_{j}$ è la frequenza massima dei termini nel documento $d_j$ e il primo fattore rappresenta la frequenza relativa del termine $t_i$ nel documento $t_j$.
