I language model, come dice il nome stesso, sono dei modelli dell’utilizzo di una lingua, di uno scritto, di un testo. Quindi un modello di un linguaggio di un testo può essere utilizzato la dove esistono dei testi, quindi per vari scopi. Sappiate che questi modelli, questa tecnica, sono utilizzati per diversi scopi: natural language processing, information retrieval e altri tipi di applicazioni che hanno a che fare con l’analisi di testi. Un documento può quindi essere rappresentato con un language model. Dall’altra parte, anche una query, o un’esigenza di informazione, può anch’essa essere rappresentata con i language model. Quindi, quando si parla i definizione di language model, come vedremo insieme, ci sono diversi categorie di modelli che noi possiamo definire per realizzare la stima della rilevanza tematica. Attenzione che qua stiamo parlando di rilevanza tematica. Abbiamo visto il modello vettoriale, abbiamo visto il modello booleano, abbiamo visto il più semplice modello probabilistico che è il Binary Independence Model, adesso vediamo un’altra categoria di modelli per la valutazione della topical relevance. Quindi il contesto è la topical relevance, la stima della topical relevance. Cos’è fondamentalmente un language model? Dal punto di vista formale, un language model è una distribuzione di probabilità. Su cosa? Sugli oggetti che costituiscono i mattoni per scrivere un testo. Quindi, i più semplici language model si basano sull’utilizzo, come rappresentanti dei testi delle parole singole. In particolare, questa categoria di language models vengono chiamati unigram language models. Cosa vuol dire? Vuol dire che per ogni parola che fa parte di un dizionario viene calcolato un valore di probabilità che quella parola sia descrittiva del topic, dell’argomento, presentato in quel documento. Questo indipendentemente dalla parola che lo precede nel testo. Immaginiamo di vedere un testo come una sequenza di parole una dopo l’altra. La modellazione di una distribuzione di probabilità che va a rappresentare il linguaggio, che viene utilizzato in un documento, dovrebbe permetterci di fare una predizione su quale sarà la prossima parola usata nel testo. Se io leggo le prime n parole, un language model, quindi una distribuzione di probabilità, dovrebbe permettermi di calcolare la probabilità di trovare una parola nella posizione n+1 esima. Negli unigram language models, per fare ciò, la parola viene considerata come un oggetto a se stante: questa probabilità non dipende dalla parole che hanno preceduto nel testo. Ci sono poi invece gli n-grams language models. Più complessi, cioè è più complesso calcolare le probabilità, ma non è impossibile, ne vedremo un esempio quando parleremo di spell-checking. Negli n-grams language models, la probabilità di vedere una parola in un testo dipende dalle n-1 parole che compaiono precedentemente. In particolare, nel bigram language model, la probabilità dipende dalla parola precedente. Adesso vi faccio una presentazione dell’unigram language model che è veramente la base dei language models. Dopodiché, quando vi spiego lo spell checking vi faccio vedere come questo può essere generalizzato. L’idea è molto semplice, l’idea è questa: immaginatevi di avere un sacchetto che contiene le vostre parole, attenzione, ogni occorrenza della parola nel testo costituisce la parola di un distinto oggetto. Quindi. Quello che si considera è di mettere in un sacchetto delle parole tutte le parole che si trovano leggendo. L’idea è quella di pensare alla generazione del testo che contiene queste parole come all’estrazione da questo sacchetto di una parola. Ne estraggo una, la leggo, rimetto quella parola nel sacchetto, passo all’estrazione della seconda. Non è che quando ne ho estratta una quella la tengo fuori, la reinseriscoo, perché per generare la parola che segue entrano in gioco tutte queste che abbiamo associato e che stiamo utilizzando. L’idea è questa. Alcune applicazioni usano bigram e trigram, più di bigram e trigram è possibile però è difficile stabilire degli n-grams language models con n maggiore di tre, dove abbiamo come abbiamo detto prima le probabilità dipendono anche dalle parole precedenti. Vi si presenta l’unigram language model perché si è dimostrato che è molto efficace. Quindi per lo scopo di stabilire un sistema di retrieval che sia basato su questo concetto di modellazione della lingua è molto utile. Visto che che vi ho già presentato la parte sulla personalizzazione della ricerca, c’è una cosa che è molto interessante fare con i language models, cioè rappresentare il modello di un utente mediante cosa? Mediante un language model. Costruito su cosa? Costruito sui testi che l’utente stesso ha generato. Quindi past queries, documenti che ha scritto e tutte quello che è stato generato dall’utente. È molto intuitivo perché rappresentiamo l’utente con quello che lui stesso ha generato. Cosa di meglio? L’idea appunto è quella di rappresentare gli argomenti, l’idea è la solita, stiamo parlando di probabilità, stiamo parlando di approcci statistici, occorrentisti. Quindi più una parola viene utilizzata all’interno di un testo, più questa sarà significativa. Cos’è questa distribuzione di probabilità dal punto di vista statistico? È una distribuzione multinomiale. Nelle distribuzioni binomiali, sostanzialmente abbiamo due possibili eventi, uno si può verificare o non si può verificare, in quelle multinomiali abbiamo diverse possibili variabili. Pensate al lancio di un dado con sei facce. Quindi multinomiali è una definizione abbastanza semplice perché abbiamo diverse parole, quindi tutte le diverse parole danno ambito a questa definizione. Quindi il testo lo vedete come una sequenza finita di parole, dove ci sono t possibili parole in ogni punto della sequenza. Vi ricordate il discorso che vi avevo fatto per il suffix-array. Vedete il testo come una sequenza: allora ogni punto, quindi terza parola quel t vale 3; decima parola, sequenza di 10 parole quel t vale 10. Che cosa vuol dire che questo tipo di modellazione è molto semplice e non tiene conto delle burstiness? In statistica, la burstiness è quel fenomeno per cui c’è la possibilità di modellare che ad un certo punto c’è un incremento della probabilità del verificarsi di un evento. Vi faccio un esempio, se io pesco una parola molto frequente e la rimetto nel bucket, al giro dopo ci sarà più alta frequenza che questa venga selezionata. Questa cosa con le distribuzioni multinomiali non viene modellata. Però non importa vengono prodotti comunque dei buoni risultati. È un modello molto semplice. Come potete immaginarvi e come vi ho detto precedentemente, modellare distribuzioni di probabilità di testi porta a tre possibilità differenti di modelli. Attenzione, cosa vuol dire probabilità di generare un testo? Se io vedo un testo come un sacchetto che contiene tutte le parole, la probabilità di generare il mio testo a partire da quelle parole è quella di estrarre le parole e calcolare le probabilità. Un testo parla di statistica, quindi la parola statistica compare molto nel testo. Qual è la probabilità che associo estraendo dal sacchetto a questa parola? In pratica potete vedere questa rappresentazione come la possibilità di calcolare la probabilità di generare delle parole nuove. Abbiamo tre categorie che corrispondono a tre distinti modelli. Prima categoria che vediamo è quella più utilizzata generalmente: probabilità di generare il testo della query, che solitamente è composta da 2 o 3 parole, dalla rappresentazione del testo di documento come language model. Quindi idea: il documento viene rappresentato come rappresentato come un languge model. Vediamo quali sono le probabilità di estrarre dal sacchetto le parole che compongono la query. Seconda categoria di modelli. È l’opposto, vado a rappresentare la quei per mezzo di un language model, di una distribuzione di probabilità e calcolo la probabilità di generare l’intero documento: è la situazione simmetrica alla prima. Terzo approccio: andiamo a rappresentare la quei come un language model, il documento come un altro language model, abbiamo due distribuzioni di probabilità, il problema è quindi quello di confrontare le distribuzioni di probabilità per vedere quanto sono prossime, quanto sono vicine. Questa è una terza categoria di modelli. Cominciamo con il query likelihood model, che sarebbe appunto la prima prospettiva, quella di calcolare la query partendo da un documento. Com’è concepito questo modello? Bisogna ottenere un ranking. L’obiettivo nella topical relevance, poi in realtà anche del calcolare la multidimensional relevance, di cui abbiamo parlato, non è quello di ottenere un valore assoluto ma ci interessa poter effettuare un ordinamento die documenti che devono essere restituiti agli utenti. Quel numero cosa mi rappresenta? Quel numero che vado a calcolare e che mi permette di costruire il ranking, che cosa mi rappresenta? Rappresenta la topical relevance. Dobbiamo ordinare i documenti sulla base della probabilità che una certa query possa essere stata generata dal modello del documento, quindi in sostanza che trattino lo stesso argomento. Vi ricordate il teorema di Bayes? Quello che a noi interessa calcolare è che data una query q il documento d sia rilevante rispetto a questa query. Noi dobbiamo calcolare la probabilità di rilevanza di un documento rispetto ad una query. Che cosa vuol dire questo “uguale rank"? Significa che quell’operazione che segue l’uguale produrrà lo stesso ordinamento, preserverà l’ordinamento. La probabilità che il documento d sia rilevante a q sarà un valore numerico, questo valore numerico cambierà per i vari documenti d, quindi avrà un valore per d1, un altro per d2, un altro per d3 e così via. Ciò che io ottengo moltiplicando queste due quantità: la probabilità di generare la query q dato un documento per la probabilità a priori del documento d. Il valore che ottengo con questa moltiplicazione, poi vedremo che semplifichiamo ulteriormente, manterrà il ranking che io ottengo in questo modo. Che cosa manca per ottenere e il teorema di Bayes? Io metterei un uguale se avessi qui un’altra componente che qui ho già eliminato, ossia diviso la probabilità di q. Vi ricordate che vi avevo spiegato che quella la assumo essere costante ed indipendente dai documenti. Possiamo anche assumere questa probabilità a priori del documento come un valore costante. In realtà la probabilità a priori del documento potremmo pensarla non costante se introduciamo la moderazione di aspetti che sono interessanti. Quali ad esempio la data di pubblicazione la data di pubblicazione, i documenti più recenti potrebbero essere più rilevanti dei documenti più vecchi. In questo caso, se un documento è più recente, la sua probabilità a priori dovrebbe essere più elevata. Non modelliamo questo genere di cose, questa moderazione è molto più semplice, quindi noi assumiamo che la probabilità a priori dei documenti sia uniforme. Quindi, sostanzialmente, noi otteniamo questa uguaglianza, che non è un’uguaglianza vera ma un’uguaglianza rispetto al ranking che otteniamo. Il calcolo della rilevanza del documento d rispetto alla query q equivale a calcolare la possibilità di generare la query q a partire dal language model che formalmente rappresenta il documento d. E questo è quello che è l’unigram. La componente che ci interessa quindi per determinare il nostro topical relevance ranking è la probabilità di generare q a partire dal documento d. La probabilità di generare q a partire da d è uguale al prodotto per i che va da 1 a n, dove n è il numero di termini della query, la probabilità che è associata al termine qi nel documento d. Che cos'è un documento d? Viene rappresentato come una distribuzione di probabilità sulle parole. Quindi cosa farò? Prendo la probabilità associata a d della parola q con i, prendo tutte le probabilità associate a tutti i termini che sono presenti nella query e faccio il prodotto di queste probabilità. Adesso vi voglio fare una domanda, come potreste calcolare le probabilità di una parola in un documento? Pensiamo a come definire questo language model che va a rappresentare il testo in un documento. Come la andiamo a calcolare la probabilità di una singola parola? È molto semplice. Una volta che io avrò memorizzato da qualche parte, dato un documento cosa dovrò fare, associare a ogni documento una distribuzione di probabilità. La domanda che vi pongo è come posso calcolare questa distribuzione di probabilità? Più nel mio sacchetto compare la stessa parola più alta sarà la probabilità che io la estragga dal mio documento. Quindi, il modo più semplice è questo: formula slide 8. Qual è il problema di questo tipo di rappresentazione? Il testo del documento è una cosa, le query sono un'altra cosa. Quindi cosa può accadere? Può accadere che una o più parole all'interno della query non siano presenti nel testo. A questo punto cosa si otterrebbe? Si otterrebbe 0 e quindi che il documento non sarà considerato rilevante rispetto a quella query. È corretto questo? No, è limitante. Se i termini della query non sono presenti nel documento, il punteggio che viene riportato sarà 0. C'è un'altra implicazione. Supponiamo di avere 5 termini che compongono la query, il fatto che ce ne sia uno solo che non è presente nel mio testo o che non ce ne sia 4 avrà lo stesso effetto, come nel modello booleano quando usiamo l'and. Se io nel modello booleano utilizzo un and, un documento potrebbe contenere alcuni elementi e non altri e avrà rilevanza 0 potrebbe non contenere alcuno degli elementi e la sua rilevanza sarà comunque 0. È corretto questo? No, perchè forse se c'è un unico termine che manca il documento potrebbe interessarci lo stesso. Quindi dobbiamo introdurre un modo per evitare questa situazione. Ricordatevi che i testi dei documenti sono un campione del language model. Quindi in realtà, le parole che non occorrono non dovrebbero avere probabilità 0; ancuni dovrebbero avere probabilità 0, altri dovrebbero avere una probabilità più alta. La tecnica che ci permette di stimare la probabilità delle parole che non sono presenti viene chiamata smoothing. La tecnica che vediamo adesso è lo smoothing. Siccome abbiamo una distribuzione di probabilità, sappiamo che questa somma ad 1. Non posso semplicemente aggiungere dei nuovi elementi con una probabilità diversa da 0. L'idea è quella di abbassare la probabilità delle parole che vengono viste, che sono effettivamente presenti nel documento, e di alzare invece, e di dare delle probabilità diverse da 0 a quelle parole che non compaiono. Quindi in pratica vuol dire fare un ribilanciamento dei valori di probabilità assunti dalle singole parole. Abbasso un po' il valore delle probabilità che ci sono e do un valore più alto a partire dal quanto ho ricavato sottraendo dai valori assoluti delle probabilità delle singole parole. Vediamo adesso come effettivamente funziona il metodo dello smoothing. La stima per le parole che non sono osservate, dei query terms che non sono osservati, nei documenti è quello che devo stimare: formula pagina 10. Sostanzialmente l'idea è: come faccio io stimare la rilevanza di una parola mancante da un documento? Vedo qual è il ruolo della parola nell'intera collezione. Quindi, per sapere quale può essere potenzialmente il ruolo di una parola all'interno di un testo, vado a vedere qual è la sua distribuzione nell'intera collezione. È un concetto che avete già visto. Quindi la probabilità di qi dato c'è la probabilità di qi nell'intera collezione. L'altro termine è la probabilità di qi dato D è la probabilità di qi nel language model. La stima per le parole che non ci sono nel documento è data da alfad per la probabilità di qi nella collezione, invece la stima della probabilità delle parole che compaiono nel documento è data dalla combinazione lineare della probabilità che la parola compaia nel documento e che compaia nell'intera collezione. Perchè questa cosa è importante? Il parametro alfa è fondamentale per mantenere la somma della distribuzione ad 1. (se un termine un termine utilizzato dalla query non compare all'interno della collezione significa che la query è sbagliata, che non può avere risposta nell'attuale collezione) Attenzione perchè il concetto dello smoothing è sintetizzato in questa slide: tenere in considerazione, modellare la probabilità delle missing words. Poi ci sono diversi altri modelli di smoothing in letteratura, ma questo è il più utilizzato. Primo modello: Jelinek-Mercer smoothing. alfad è una costante, quindi per ogni documento abbiamo lo stesso valore di alfa, quindi valore costante. Io voglio calcolare la probabilità che un termine della query viene generato dal language model del documento è data da questa combinazione lineare: pagina 11. Vedete che al posto della prima probabilità, cioè dei termini che compaiono nel testo, ho sostituito la semplice modellazione: numero di occorrenze fratto numero totale delle parole nel testo + ... Evidentemente, se diamo a lambda un valore più alto cosa accade? Diamo più importanza al fatto che una parola non compaia nei documenti, se do un valore più basso a lamda do meno importanza a questa cosa. Prima di procedere con el