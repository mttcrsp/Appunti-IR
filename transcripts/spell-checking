Il problema dello spell-checking: quando inseriamo delle parole con una tastiera commettiamo degli errori. Se lo facciamo utilizzando i motori di ricerca, questo potrebbe innervosire gli utenti, perché se il motore di ricerca restituisse delle risposte vuote, voi non sareste per niente contenti. Sono quindi state messe a punto delle tecniche, che prevedono, in caso di una parola inserita trovata nel dizionario, vi danno una serie di suggerimenti. E voi questa modalità di suggerimenti l’avete già vista. Su che cosa è basata? Le parole vengono considerate e vengono comparate, confrontate con delle parole che sono corrette e sono state memorizzate nel dizionario.

Abbiamo detto l’altra volta che l’aspetto chiave è quello di definire una misura di similarità lessicale, Abbiamo già detto che le edit distances sono quelle che vengono utilizzate per lo più, combinate ad altre tecniche che oggi vi presento, l’edita distance praticamente non è nient’altro che un conteggio dei caratteri che generano la differenza tra due sequenze alfanumeriche di parole. Quella che viene utilizzata di più è la Levehnstein distance.

Abbiate presente che un motore di ricerca l’efficienza deve essere totale per non perdersi i clienti. Non è solo una questione di efficacia ma anche di efficienza. Devono essere velocissimi nel proporre delle soluzioni. Si fa un confronto della parola che è stata scritta in modo sbagliato con tutto il dizionario? Come si fanno a selezionare le parole candidate ad essere delle correzioni della parola sbagliata? Si prendono le parole con più o meno la stessa lunghezza e il fatto di iniziare allo stesso modo, perché la maggior parte degli errori si verificano non all’inizio della parola, ma piuttosto in mezzo.

E poi c’è un’altra tecnica che vi spiego oggi, l’utilizzo di questo soundex code. Quindi, il problema importante da un punto di vista computazionale è quello di fare una word restriction, di ridurre l’insieme dei potenziali candidati ad essere delle soluzioni. Poi vedremo anche degli aspetti bellissimi che riguardano i language models, e li vediamo applicati a questo contesto. Un altro modo è quello di considerare parole che hanno lo stesso suono. In pratica il soundex code è un codice fonetico che raggruppa, clusterizza, parole sulla base della loro similarità fonetica di pronuncia, solitamente queste tre tecniche vengono usate tutte, e vengon combinate per ottenere una maggiore restrizione delle soluzioni candidate. L’algoritmo è il seguente: mantieni la prima lettera della parola, al posto delle vocali e di H e W metti degli hyphen, e poi rimpiazza le altre lettere con dei numeri. Chiaramente c’è un senso dietro a queste scelte. A certe categorie di consonanti viene sostituito uno, ad altre due e così via. Poi, le ripetizioni dello stesso carattere in posizioni adiacenti vengono cancellate, e anche le occorrenze adiacenti di numeri vengono cancellate. Dopodiché vengono rimosse le virgolette utilizzate come puntatori ai caratteri che devono essere rimossi. Il codice da cosa è rappresentato? Dai primi tre numeri ottenuti. Se non ci sono abbastanza numeri si va a riempire la parte finale con degli zeri.

Qual è la caratteristica che viene implementata nei motori di ricerca? “Did you mean”. È quindi importante anche implementare un ordinamento, ossia che la prima parola che vi viene suggerita sia quella che nel novanta per cento dei casi rappresenta la vera correzione. Per generare un ranking, cosa bisogna fare? Per ordinare dobbiamo avere a disposizione un numero, come facciamo a calcolare questo numero? La distanza di edit può essere una parte ma non ha quell’accuratezza che ci consente di fare pochi errori, perché se lei ci pensa la distanza di edit catturerebbe un sacco di parole anche fuori contesto. In pratica, dobbiamo avere delle sorgenti di evidenza che una cerca parola possa essere una correzione dell’altra, cosa abbiamo a disposizione? Abbiamo a disposizione ad esempio tutte le query che un utente ha già formulato, se una parola compare spesso con altre parole e il nostro errore compare con le stesse parole, la probabilità che la correzione sia quella è alta. Quindi anche in questo caso gli approcci sono probabilistici. Andiamo a valutare la probabilità che una parola scritta in modo sbagliato, la sua correzione sia un’altra parola presa dal sottoinsieme del dizionario. Inoltre, esistono due tipi di modelli per calcolare questa probabilità: uno totalmente avulso dal contesto, attenzione che in questo caso la parola contesto è usata una maniera del tutto diversa dal campo della ricerca contestuale, in ambito linguistico invece il contesto di un parola è rappresentato dalle parole che la circondando, quindi la valutazione della probabilità funziona molto meglio se so la parola che ha preceduto quella che ho sbagliato. Se io osservo l’utente, la prima parola che ha inserito è corretta, la seconda è sbagliata, la conoscenza dell’errore, della parola corretta prima dell’errore può essere fondamentale per disambiguare il senso della parola che ho sbagliato. Possiamo quindi considerare finestre di contesto rappresentate da unigrammi, come potremmo anche considerare finestre di contesto anche più ampie, ma tipicamente mai maggiori di due. Ci sono altri tipi di errori che sono quelli in cui ci dimentichiamo di inserire uno spazio. Nel contesto degli algoritmi appena illustrati bisogna tenere conto anche di questa situazione. Adesso vi presento il modello che ci permette di generare una stima della probabilità della parola che corregge l’errore. Non confondete le due parti. Applicare l’edit distance o il soundex code è la fase preliminare, selezioniamo dei candidati, una volta che abbiamo dei candidati li ordiniamo in ordine decrescente di probabilità di essere le parole corrette.

NOISY CHANNEL MODEL

L’assunzione è questa: un utente sceglie una parola sulla base di una distribuzione di probabilità, quindi ogni parola avrà una certa probabilità di essere inserita come keyword in un motore di ricerca, dobbiamo costruire un language model che va a costituire la probabilità di inserire certi descrittori a livello di formulazione di query. Per catturare un’informazione di contesto abbiamo una probabilità condizionata P(w1|w2), l’idea di questo modello, chiamato Noisy Channel Model, è che l’utente scrive una parola, per il noisy channel rappresentato dall’interazione con la tastiera causa un errore. L’error model, il modello che mi permette di ottenere un valore numerico per ogni parola sbagliata è questo: qual è la probabilità che la parola w1 sia data da un misstyping della parola w2. Cosa va a rappresentare questa distribuzione di probabilità? Rappresenta la distribuzione di probabilità legata agli errori di spelling. Tutto quello che è conoscenza sul query log è fondamentale per generare questo modello. Noi abbiamo il language model del linguaggio degli utenti che formulano una query e questa è una distribuzione di probabilità assoluta P(w), non è condizionata. Poi abbiamo un modello P(e|w) che ci dice quel è la frequenza degli errori e li associa alle parole. Il problema che vogliamo risolvere è: data una parola e qual è la probabilità che la parola intesa sia la parola w, ossia P(w|e). Questa probabilità si ottiene con il prodotto P(w|e) = P(e|w)P(w). Qual è il senso dietro a tutto questo? Un errore di battitura sarà tanto più frequente quanto è più frequente la parola che dovrebbe essere specifica dall’utente, quindi che sia proporzionale alla probabilità a priori della parola dovrebbe essere evidente, più una parola viene utilizzata dagli utenti più aumenta la parola che l’utente volesse catturare fosse proprio quella. Naturalmente questa probabilità deve essere combinata con la probabilità che la parola inserita sia un errore generato nel tentativo di specificare la parola corretta.

Complichiamo la cosa, la distribuzione di probabilità delle parole, che possiamo estrarre dai query log, è banale, non vado a considerare il contesto, invece è molto utile considerare il contesto: questo è il contesto alla language model. Come viene espressa la probabilità di una parola tenendo conto della parola che è stata specificata precedentemente. Se precedentemente non è stato specificato nulla, la probabilità dipenderà solo dalla parola e dall’analisi dei query log, è una combinazione lineare P(w) = λP(w) + (1 − λ)P(w|wp). Questo vuol dire che la probabilità di una parola è data da una combinazione lineare della probabilità della parola da sola, che andiamo a conteggiare quante volte negli ultimi n anni quella parola è stata utilizzata più la probabilità che la parola sia stata digitata immediatamente dopo la parola presentata. Se voglio dare più importanza al contesto utilizzerò valori di λ piccoli. Posso giocare su quale sia il valore più importante, se il contesto o quello della parola a priori.

Le probabilità utilizzate per la costruzione dei language model vengono stimate a partire dai documenti e dai query log. Per stimare l’errore model, l’approccio più cieco, che non va a guardare i query log, è quello di assumere che tutte le parole che hanno una certa edit distance abbiano la stessa probabilità di essere la correzione di una parola. Questa è l’assunzione più semplice ma anche meno accurata. Oppure approcci più complessi vanno ad incorporare informazioni sugli errori di battitura più comuni. Questo è fortemente influenzato dalla tastiera che si usa.

Quindi in pratica lo spell-check process è basato su questa sequenza di passi che abbiamo visto, riassunti alla slide 23.

Esistono delle tecniche di query expansion totalmente automatiche. I metodi globali vanno ad indagare le ricerche precedenti per verificare se sono state proposte in precedenza della ricerche che utilizzavano un soprainsieme dei termini utilizzati dalla query corrente. Questi approcci sono particolarmente inefficienti dal punto di vista computazionale. I metodi locali effettuano l’espansione a partire dalla sessione di query stessa, ossia dalle interazioni successive alla prima query. Le semplificazioni che vengono fatte per implementare questi metodi sono molte, vengono considerate i top k results, i primi 10 risultati prodotti dalla query. Il motore di ricerca, presuntuosamente, assume che i primi 3/4 risultati siano quelli di maggiore interesse per l’utente, li analizza e le parole maggiormente presenti all’interno di questi documenti le utilizza per espandere la query. Questa tecnica poteva andare bene quando non si faceva diversificazione dei risultati.

L’espansione basata sull’utilizzo di tesauri non è efficace. I web search engine trattano argomenti diversi, quindi avere un tesauro su tutto lo scibile non è possibile o se lo si ha a disposizione non è sicuramente accurato.